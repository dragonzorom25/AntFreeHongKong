# -*- coding: utf-8 -*-
"""
ğŸ“˜ athena_k_market_ai_prod.py (v1.1)
--------------------------------------------
âœ… í•œêµ­ ì£¼ì‹ ì‹œì¥ ë°ì´í„° ë¶„ì„ ë° ê¸°ìˆ ì  íŒ¨í„´ ê°ì§€ ìŠ¤í¬ë¦½íŠ¸
    - ê¸°ëŠ¥: ì¢…ëª© ë¶„ì„ í•„í„°ë§ (analyze ëª¨ë“œ), ì°¨íŠ¸ ì‹œê°í™” ë°ì´í„° ìƒì„± (chart ëª¨ë“œ)
    - ìˆ˜ì •: --symbol ì¸ìë¥¼ í†µí•œ ë‹¨ì¼ ì¢…ëª© ë¶„ì„ ê¸°ëŠ¥ ì¶”ê°€
    - ì¶”ê°€: 'half_cup' (ê·¸ë¦‡ í—ˆë¦¬) ë¡œì§ ìœ ì§€
    - ì¶”ê°€: 'long_term_down_trend' (ì¥ê¸° í•˜ë½ ì¶”ì„¸) ë¡œì§ ì¶”ê°€
    - ì›ë³¸ ìœ ì§€: 700í–‰ ì´ìƒì˜ ë°©ëŒ€í•œ ì˜ˆì™¸ ì²˜ë¦¬ ë° ë¡œê¹… ë¡œì§ ì „ì²´ ë³µêµ¬
"""

import os
import sys
import json
import time
import logging
import argparse
import traceback
import socket
from pathlib import Path
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
import glob

import pandas as pd
import numpy as np
from scipy.signal import find_peaks
import ta
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans


# ==============================
# 1. ì´ˆê¸° ì•ˆì „ ê²€ì‚¬ ë° í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
# ==============================

def safe_print_json(data, status_code=1):
    """í‘œì¤€ ì¶œë ¥(stdout)ìœ¼ë¡œ JSONì„ ì•ˆì „í•˜ê²Œ ì¶œë ¥í•˜ê³  í”„ë¡œì„¸ìŠ¤ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤."""
    try:
        # CustomJsonEncoderë¥¼ ì‚¬ìš©í•˜ì—¬ np íƒ€ì… ë° datetime ê°ì²´ ì²˜ë¦¬
        sys.stdout.write(json.dumps(data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=CustomJsonEncoder) + "\n")
    except Exception as e:
        sys.stdout.write(json.dumps({"error": "JSON_SERIALIZATION_FAIL", "original_error": str(e)}, ensure_ascii=False) + "\n")
        
    sys.stdout.flush()
    if status_code != 0:
        sys.exit(status_code)

def check_internet_connection(host="8.8.8.8", port=53, timeout=3):
    """ê°„ë‹¨í•œ ì†Œì¼“ ì—°ê²°ì„ í†µí•´ ì¸í„°ë„· ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤."""
    try:
        socket.setdefaulttimeout(timeout)
        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        s.connect((host, port))
        s.close()
        return True
    except Exception:
        return False

# ìŠ¤í¬ë¦½íŠ¸ ì‹œì‘ ì‹œ ì¸í„°ë„· ì—°ê²° í™•ì¸
if not check_internet_connection():
    safe_print_json({"error": "CRITICAL_ERROR", "reason": "ì¸í„°ë„· ì—°ê²°ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "mode": "initial_check"})

# ==============================
# 1.5. JSON Custom Encoder ì •ì˜
# ==============================
class CustomJsonEncoder(json.JSONEncoder):
    """NumPy íƒ€ì… ë° Pandas Timestampë¥¼ í‘œì¤€ Python íƒ€ì…ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    def default(self, obj):
        if isinstance(obj, np.bool_):
            return bool(obj)
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        if isinstance(obj, (np.floating, np.float64, np.float32)):
            if np.isnan(obj):
                return None
            return float(obj)
        if isinstance(obj, set):
            return list(obj)
        if isinstance(obj, (pd.Timestamp, datetime, np.datetime64)):
            return obj.strftime('%Y-%m-%d')
        return json.JSONEncoder.default(self, obj)


# ==============================
# 2. ê²½ë¡œ ë° ìƒìˆ˜ ì„¤ì •
# ==============================
# â†’ ìƒìœ„ 2ë‹¨ê³„ë¡œ ì˜¬ë¼ê°€ë©´ /MyBaseLinkV2/python
BASE_DIR = Path(__file__).resolve().parents[2]
LOG_DIR = BASE_DIR / "log"
DATA_DIR = BASE_DIR / "data" / "stock_data" 
LISTING_FILE = BASE_DIR / "data" / "stock_list" / "stock_listing.json" 
CACHE_DIR = BASE_DIR / "cache" 
LOG_FILE = LOG_DIR / "stock_analyzer_ultimate.log"


# ==============================
# 3. í™˜ê²½ ì´ˆê¸°í™” ë° ìœ í‹¸ë¦¬í‹°
# ==============================

def setup_env(log_level=logging.INFO):
    """í™˜ê²½ ë””ë ‰í† ë¦¬ë¥¼ ì„¤ì •í•˜ê³  ë¡œê¹…ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
    LOG_DIR.mkdir(parents=True, exist_ok=True)
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    LISTING_FILE.parent.mkdir(parents=True, exist_ok=True)
    CACHE_DIR.mkdir(parents=True, exist_ok=True)
    
    logging.basicConfig(
        level=log_level,
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        handlers=[
            logging.FileHandler(LOG_FILE, encoding="utf-8", mode='a'),
            logging.StreamHandler(sys.stdout)
        ]
    )

def load_listing():
    """ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ (stock_listing.json)ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    default_item = [{"Code": "005930", "Name": "ì‚¼ì„±ì „ì"}]
    if not LISTING_FILE.exists():
        logging.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ì—†ìŒ: {LISTING_FILE}")
        return default_item
    try:
        with open(LISTING_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logging.error(f"ì¢…ëª© ë¦¬ìŠ¤íŠ¸ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return default_item

def get_stock_name(symbol):
    """ì¢…ëª© ì½”ë“œë¡œ ì´ë¦„ì„ ì°¾ì•„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    try:
        items = load_listing()
        for item in items:
            code = item.get("Code") or item.get("code")
            if code == symbol: return item.get("Name") or item.get("name")
        return symbol
    except Exception: return symbol

# ìºì‹œ ì •ë¦¬ í•¨ìˆ˜
def cleanup_old_cache(days=7):
    """ì§€ì •ëœ ê¸°ê°„(ì¼)ë³´ë‹¤ ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ì„ ì‚­ì œí•©ë‹ˆë‹¤."""
    logging.info(f"ë§Œë£Œëœ ({days}ì¼ ì´ìƒ) ìºì‹œ íŒŒì¼ ì •ë¦¬ ì‹œì‘.")
    
    cutoff_time = datetime.now() - timedelta(days=days)
    
    cache_files = CACHE_DIR.glob('*.json')
    
    deleted_count = 0
    for file_path in cache_files:
        try:
            mod_time = datetime.fromtimestamp(file_path.stat().st_mtime)
            
            if mod_time < cutoff_time:
                file_path.unlink()  
                deleted_count += 1
                logging.debug(f"ìºì‹œ íŒŒì¼ ì‚­ì œ: {file_path.name}")
        except Exception as e:
            logging.error(f"ìºì‹œ íŒŒì¼ {file_path.name} ì •ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

    logging.info(f"ì´ {deleted_count}ê°œì˜ ì˜¤ë˜ëœ ìºì‹œ íŒŒì¼ì„ ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.")


# =====================================================================================
# ê³ ê¸‰ íŠ¹ì§• ê³µí•™ ë° í´ëŸ¬ìŠ¤í„°ë§ ë¡œì§
# =====================================================================================

def calculate_advanced_features(df: pd.DataFrame) -> pd.DataFrame:
    """ê³ ê¸‰ íŒ¨í„´ ì¸ì‹ì„ ìœ„í•´ ê¸°ìˆ ì  ì§€í‘œë¥¼ íŠ¹ì§•(Feature)ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤."""
    df['RSI'] = ta.momentum.RSIIndicator(close=df['Close'], window=14, fillna=False).rsi()
    df['MACD'] = ta.trend.MACD(close=df['Close'], fillna=False).macd()
    df['MACD_Signal'] = ta.trend.MACD(close=df['Close'], fillna=False).macd_signal()
    df['MACD_Hist'] = ta.trend.MACD(close=df['Close'], fillna=False).macd_diff() 

    bollinger = ta.volatility.BollingerBands(close=df['Close'], window=20, window_dev=2, fillna=False)
    df['BB_Width'] = bollinger.bollinger_wband()

    df['SMA_20'] = ta.trend.SMAIndicator(close=df['Close'], window=20, fillna=False).sma_indicator()
    df['SMA_50'] = ta.trend.SMAIndicator(close=df['Close'], window=50, fillna=False).sma_indicator()
    df['SMA_200'] = ta.trend.SMAIndicator(close=df['Close'], window=200, fillna=False).sma_indicator()

    df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))
    df['TREND_CROSS'] = (df['SMA_50'] > df['SMA_200']).astype(int)

    feature_subset = ['RSI', 'MACD', 'BB_Width', 'TREND_CROSS', 'SMA_200', 'Log_Return']
    df_with_features = df.copy().dropna(subset=feature_subset)
    return df_with_features

def add_market_regime_clustering(df_full: pd.DataFrame, n_clusters=4) -> pd.DataFrame:
    """K-Means í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•´ ì‹œì¥ êµ­ë©´(Market Regime)ì„ ì •ì˜í•˜ê³  í• ë‹¹í•©ë‹ˆë‹¤."""
    feature_cols = ['RSI', 'MACD', 'BB_Width', 'TREND_CROSS', 'Log_Return'] 
    min_data_length = 200

    if len(df_full) < min_data_length or not all(col in df_full.columns for col in feature_cols):
        df_full['MarketRegime'] = -1 
        return df_full

    data = df_full[feature_cols].copy()

    if data.drop_duplicates().shape[0] < n_clusters:
        df_full['MarketRegime'] = -1 
        return df_full
    
    scaler = StandardScaler()
    scaled_data = scaler.fit_transform(data) 

    try:
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10, init='k-means++')
        df_full['MarketRegime'] = kmeans.fit_predict(scaled_data) 
    except ValueError as e:
        df_full['MarketRegime'] = -1

    return df_full


# =====================================================================================
# 1. MAì¥ê¸°í•˜ë½ì¶”ì„¸ ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ë¡œì§ 
# =====================================================================================
def find_long_term_down_trend(df):
    """
    â˜… í˜•ë‹˜ì˜ 'MA ì¥ê¸° í•˜ë½ ì¶”ì„¸': 
    - 200ì¼ì„ ì´ ì™„ë²½í•˜ê²Œ ìš°í•˜í–¥í•˜ê³ , ì£¼ê°€ê°€ ê·¸ ì•„ë˜ì—ì„œ ê³„ì† ì²˜ë°•íˆëŠ” ì¢…ëª©
    """
    if len(df) < 250: return False, 0, 'None', 0
    
    curr = df.iloc[-1]
    prev_20 = df.iloc[-20]
    
    # 200ì¼ì„  ìš°í•˜í–¥ ì—¬ë¶€
    is_ma200_down = curr['SMA_200'] < prev_20['SMA_200']
    # ì™„ì „ ì—­ë°°ì—´ (í˜„ì¬ê°€ < 20 < 50 < 200)
    is_perfect_reverse = (curr['Close'] < curr['SMA_20'] < curr['SMA_50'] < curr['SMA_200'])
    # 200ì¼ì„ ê³¼ì˜ ì´ê²©ë„ (ì–¼ë§ˆë‚˜ ë§ì´ ë–¨ì–´ì¡Œë‚˜)
    drop_dist = (curr['SMA_200'] - curr['Close']) / curr['SMA_200']
    
    if is_ma200_down and is_perfect_reverse and drop_dist > 0.20:
        # ì •ë ¬ ì ìˆ˜ë¡œ í™œìš©í•  ì´ê²©ë„(%) ë°˜í™˜
        return True, curr['SMA_200'], 'Downward', drop_dist * 100
        
    return False, 0, 'None', 0
    
def find_peaks_and_troughs(df, prominence_ratio=0.005, width=3):
    """ì£¼ìš” ë´‰ìš°ë¦¬(Peaks)ì™€ ê³¨ì§œê¸°(Troughs) ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤ (ìµœê·¼ 250ì¼ ê¸°ì¤€)."""
    recent_df = df.iloc[-250:].copy()
    if recent_df.empty: return np.array([]), np.array([])
    # Note: Use a fixed window for std to prevent instability if data changes often
    std_dev = recent_df['Close'].std() 
    prominence_val = std_dev * prominence_ratio 
    
    peaks, _ = find_peaks(recent_df['Close'], prominence=prominence_val, width=width)
    troughs, _ = find_peaks(-recent_df['Close'], prominence=prominence_val, width=width)
    
    start_idx = len(df) - len(recent_df)
    return peaks + start_idx, troughs + start_idx


# =====================================================================================
# 2. ì´ì¤‘ ë°”ë‹¥ ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ë¡œì§ 
# =====================================================================================

def find_double_bottom(df, troughs, tolerance=0.02, min_duration=20):
    """
    ìµœê·¼ 1ë…„ ì¤‘ ìµœì €ê°€ ë¶€ê·¼(5% ì´ë‚´)ì—ì„œë§Œ í˜•ì„±ëœ ê¼­ì§“ì ë§Œ ì¸ì •í•©ë‹ˆë‹¤.
    """
    if len(df) < 250: return False, None, 'None', 0
    
    recent_df = df.iloc[-250:]
    absolute_low = recent_df['Low'].min() # ìµœê·¼ 1ë…„ ì „ì²´ ìµœì €ì 
    
    recent_troughs = [t for t in troughs if t >= len(df) - 150]
    if len(recent_troughs) < 2: return False, None, 'None', 0
    
    idx2, idx1 = recent_troughs[-1], recent_troughs[-2]
    p1, p2 = df['Close'].iloc[idx1], df['Close'].iloc[idx2]
    
    # [í•µì‹¬] ê¼­ì§“ì ì´ ê±°ì˜ ë°”ë‹¥ì¸ê°€? (ìµœì €ê°€ ëŒ€ë¹„ 5% ì´ë‚´ë§Œ í—ˆìš©)
    is_at_absolute_bottom = (p1 <= absolute_low * 1.05) and (p2 <= absolute_low * 1.05)
    if not is_at_absolute_bottom: return False, None, 'None', 0

    # ë°”ë‹¥ë¼ë¦¬ì˜ ê°€ê²© ì¼ì¹˜ì„± (ì˜¤ì°¨ 2% ì´ë‚´ë¡œ ì´ˆì •ë°€)
    if abs(p1 - p2) / min(p1, p2) > tolerance: return False, None, 'None', 0
    
    interim_high = df['Close'].iloc[idx1:idx2].max()
    current_price = df['Close'].iloc[-1]
    
    # ë„¥ë¼ì¸ ê·¼ì²˜ì—ì„œ ì´ì œ ë§‰ ê³ ê°œ ë“œëŠ” ì¢…ëª©ë§Œ
    if interim_high * 0.85 <= current_price <= interim_high * 1.10:
        return True, interim_high, 'Potential', interim_high
    
    return False, None, 'None', 0


# =====================================================================================
# 3. ì‚¼ì¤‘ ë°”ë‹¥ ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ë¡œì§ 
# =====================================================================================
def find_triple_bottom(df, troughs, tolerance=0.03):
    """
    ì„¸ ê°œì˜ ë¹¨ê°„ ë™ê·¸ë¼ë¯¸ê°€ ëª¨ë‘ 1ë…„ ìµœì €ê°€ ìˆ˜ì¤€ì— ì •ë ¬ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
    """
    if len(df) < 250: return False, None, 'None', 0
    
    recent_df = df.iloc[-250:]
    absolute_low = recent_df['Low'].min()
    
    recent_troughs = [t for t in troughs if t >= len(df) - 200]
    if len(recent_troughs) < 3: return False, None, 'None', 0
    
    idx3, idx2, idx1 = recent_troughs[-1], recent_troughs[-2], recent_troughs[-3]
    prices = [df['Close'].iloc[idx1], df['Close'].iloc[idx2], df['Close'].iloc[idx3] ]
    
    # [í•µì‹¬] ì„¸ ê¼­ì§“ì  ëª¨ë‘ ë°”ë‹¥ê¶Œì¸ê°€?
    if not all(p <= absolute_low * 1.07 for p in prices): return False, None, 'None', 0
    
    # ì„¸ ë°”ë‹¥ì˜ ìˆ˜í‰ ìœ ì§€ (í˜•ë‹˜ ê·¸ë¦¼ì²˜ëŸ¼ ì¼ì§ì„ )
    if (max(prices) - min(prices)) / min(prices) > tolerance: return False, None, 'None', 0
    
    neckline = df['Close'].iloc[idx1:idx3].max()
    current_price = df['Close'].iloc[-1]
    
    if neckline * 0.8 <= current_price <= neckline * 1.15:
        return True, neckline, 'Potential', neckline
    
    return False, None, 'None', 0

# =====================================================================================
# 4. ì»µ ì•¤ í•¸ë“¤ ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ë¡œì§ 
# =====================================================================================

def find_cup_and_handle(df, peaks, troughs, handle_drop_ratio=0.3):
    """
    â˜… í˜•ë‹˜ ì „ìš© ì»µ ì•¤ í•¸ë“¤ (Early-Stage Cup)
    ì´ë¯¸ ì™„ì„±ëœ ì»µì€ ì œì™¸í•˜ê³ , ê¸‰ë½ í›„ ë°”ë‹¥ì„ ë‹¤ì§„ ë’¤ 
    ì´ì œ ë§‰ 'ì˜¤ë¥¸ìª½ ì†ì¡ì´'ë¥¼ ë§Œë“¤ë ¤ëŠ” ì´ˆê¸° ì¢…ëª©ì„ í¬ì°©í•©ë‹ˆë‹¤.
    """
    if len(df) < 250: return False, None, 'None', 0
    
    recent_250 = df.iloc[-250:]
    # 1. ì»µì˜ ì‹œì‘ì  (ê¸‰ë½ ì „ ì–¸ë•)ê³¼ ë°”ë‹¥ì  í™•ì¸
    peak_price = recent_250['High'].max()     # ì»µì˜ ì™¼ìª½ ë
    trough_price = recent_250['Low'].min()    # ì»µì˜ ë°”ë‹¥
    current_price = df['Close'].iloc[-1]
    
    # 2. ì»µì˜ ê¹Šì´ ê²€ì¦ (í˜•ë‹˜ ê·¸ë¦¼ì²˜ëŸ¼ ê¹Šê²Œ íŒŒì—¬ì•¼ í•¨)
    cup_depth_pct = (peak_price - trough_price) / peak_price
    if cup_depth_pct < 0.30: return False, None, 'None', 0
    
    # 3. 'Uì'ê°€ ë˜ê¸° ì „ ì‹œì‘ ì§€ì  í¬ì°© (íšŒë³µë¥  30% ~ 60% êµ¬ê°„)
    # ì „ê³ ì ì„ ëš«ìœ¼ëŸ¬ ê°€ëŠ” 80~90% êµ¬ê°„ì€ ë„ˆë¬´ ëŠ¦ì—ˆìœ¼ë¯€ë¡œ ë°°ì œ
    recovery_rate = (current_price - trough_price) / (peak_price - trough_price)
    
    # í˜•ë‹˜ ê·¸ë¦¼íŒì˜ "Uìê°€ ë˜ê¸° ì „" êµ¬ê°„ (í•¸ë“¤ í˜•ì„± ì´ˆì…)
    is_early_cup = (0.25 <= recovery_rate <= 0.55)
    
    if is_early_cup:
        # ìµœê·¼ 20ì¼ê°„ì˜ ì›€ì§ì„ì´ ë°”ë‹¥ì„ íƒˆì¶œí•˜ì—¬ ì™„ë§Œí•œ ìƒìŠ¹ ê³¡ì„ ì„ ê·¸ë¦¬ëŠ”ì§€ í™•ì¸
        sma20 = df['SMA_20'].iloc[-1]
        sma50 = df['SMA_50'].iloc[-1]
        
        # 20ì¼ì„ ì´ ê³ ê°œë¥¼ ë“¤ê³  ì£¼ê°€ê°€ ê·¸ ìœ„ì— ì•ˆì°©í–ˆì„ ë•Œê°€ ì° í•¸ë“¤ ìë¦¬
        if current_price > sma20:
            # ë­í‚¹ ì ìˆ˜: ì»µì´ ê¹Šê³  íš¡ë³´ê°€ ì ì ˆí–ˆì„ìˆ˜ë¡ ë†’ì€ ì ìˆ˜
            c_score = (cup_depth_pct * 100) + (recovery_rate * 50)
            return True, peak_price, 'Potential', c_score

    return False, None, 'None', 0

# =====================================================================================
# 5. í•˜í”„ì•¤í•¸ë“¤ ê¸°ìˆ ì  ë¶„ì„ íŒ¨í„´ ë¡œì§ 
# =====================================================================================
def find_half_cup_waist(df):
    """
    â˜… í˜•ë‹˜ì˜ 'Lìí˜• ë°”ë‹¥ íƒˆì¶œ' ê·¸ë¦¼ ë°˜ì˜
    - ìˆ˜ì •: í•˜ë½í­ì´ í¬ê³  ë°”ë‹¥ íš¡ë³´ê°€ ê¸¸ìˆ˜ë¡ ë†’ì€ ë­í‚¹ ì ìˆ˜(l_score) ë¶€ì—¬
    """
    if len(df) < 250: return False, None, 'None', 0
    
    recent_250 = df.iloc[-250:]
    peak_price = recent_250['High'].max()     # ì „ê³ ì 
    trough_price = recent_250['Low'].min()    # ìµœì €ì 
    current_price = df['Close'].iloc[-1]
    
    total_drop = peak_price - trough_price
    if total_drop <= 0: return False, None, 'None', 0
    
    total_drop_pct = total_drop / peak_price
    recovery_rate = (current_price - trough_price) / total_drop
    
    # 1. ì° ë°”ë‹¥ êµ¬ê°„ í•„í„°ë§ (íšŒë³µë¥  10%~35%ë¡œ ë” ë³´ìˆ˜ì ìœ¼ë¡œ ì¡ìŒ)
    is_waist_zone = (0.10 <= recovery_rate <= 0.35) 
    is_not_complete_cup = (current_price < peak_price * 0.70)
    
    if is_waist_zone and is_not_complete_cup:
        # íš¡ë³´ì„± ê³„ì‚°: ìµœê·¼ 40ì¼ê°„ ë°”ë‹¥ê¶Œì— ë¨¸ë¬¸ ë¹„ìœ¨
        bottom_threshold = trough_price * 1.15
        days_at_bottom = (recent_250['Close'].iloc[-40:] <= bottom_threshold).sum()
        
        # â˜… L-Score: (í•˜ë½ë¥  * 100) + (íš¡ë³´ì¼ìˆ˜ * 2) -> ì´ê²Œ ë†’ì„ìˆ˜ë¡ ì •ë ¬ ìƒë‹¨
        l_score = (total_drop_pct * 100) + (days_at_bottom * 2)
        
        sma20 = df['SMA_20'].iloc[-1]
        # ê³ ê°œ ì‚´ì§ ë“¤ê¸° (20ì¼ì„  ê·¼ì²˜)
        if current_price > sma20 * 0.98:
            return True, peak_price, 'Potential', l_score
             
    return False, None, 'None', 0
    
# =====================================================================================
# 6. ê¸°ìˆ ì  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„
# =====================================================================================

def check_ma_conditions(df, periods, analyze_patterns):
    """ì´ë™ í‰ê· ì„  ì¡°ê±´ ë° íŒ¨í„´ ë¶„ì„ì„ ìˆ˜í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë”•ì…”ë„ˆë¦¬ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤."""
    results = {}
    ma_cols = {20: 'SMA_20', 50: 'SMA_50', 200: 'SMA_200'}

    if len(df) < 200: analyze_patterns = False

    # 1. ì£¼ê°€ì™€ MA ë¹„êµ
    for p in periods:
        col_name = ma_cols.get(p)
        if col_name and col_name in df.columns and not df.empty:
            results[f"above_ma{p}"] = df['Close'].iloc[-1] > df[col_name].iloc[-1]
        else:
            results[f"above_ma{p}"] = False

    # 2. ê³¨ë“ /ë°ë“œ í¬ë¡œìŠ¤ ê°ì§€ (50ì¼ì„  vs 200ì¼ì„ )
    ma50_col = ma_cols.get(50)
    ma200_col = ma_cols.get(200)

    if ma50_col in df.columns and ma200_col in df.columns and len(df) >= 200:
        ma50_prev, ma50_curr = df[ma50_col].iloc[-2], df[ma50_col].iloc[-1]
        ma200_prev, ma200_curr = df[ma200_col].iloc[-2], df[ma200_col].iloc[-1]

        results["goldencross_50_200_detected"] = (ma50_prev < ma200_prev and ma50_curr > ma200_curr)
        results["deadcross_50_200_detected"] = (ma50_prev > ma200_prev and ma50_curr < ma200_curr)
    else:
        results["goldencross_50_200_detected"] = False
        results["deadcross_50_200_detected"] = False

    # 3. ê¸°ìˆ ì  íŒ¨í„´ ë¶„ì„ 
    if analyze_patterns:
        peaks, troughs = find_peaks_and_troughs(df)
        
        _, _, db_status, db_price = find_double_bottom(df, troughs)
        _, _, tb_status, _ = find_triple_bottom(df, troughs)
        _, _, ch_status, ch_price = find_cup_and_handle(df, peaks, troughs)
        
        # â˜… í—ˆë¦¬ êµ¬ê°„ ê°ì§€ (ì»µ ëª¨ì–‘ ì œì™¸)
        _, _, hc_status, l_score = find_half_cup_waist(df)
        
        # â˜… ì¥ê¸° í•˜ë½ ì¶”ì„¸ ì¶”ê°€
        _, _, ltd_status, ltd_score = find_long_term_down_trend(df)

        results['pattern_double_bottom_status'] = db_status
        results['db_neckline_price'] = db_price

        results['pattern_triple_bottom_status'] = tb_status

        results['pattern_cup_and_handle_status'] = ch_status
        results['ch_neckline_price'] = ch_price
        
        # â˜… í—ˆë¦¬ êµ¬ê°„ ê²°ê³¼ ì¶”ê°€
        results['pattern_half_cup_status'] = hc_status
        results['hc_l_score'] = l_score # ì •ë ¬ìš© ì ìˆ˜ ë³´ê´€
        
        # â˜… ì¥ê¸° í•˜ë½ ê²°ê³¼ ì¶”ê°€
        results['pattern_long_term_down_trend_status'] = ltd_status
        results['ltd_score'] = ltd_score

    # 4. ì‹œì¥ êµ­ë©´ (Market Regime)
    if 'MarketRegime' in df.columns and not df.empty:
        results['market_regime'] = int(df['MarketRegime'].iloc[-1])
    else:
        results['market_regime'] = -1

    return results


# ==============================
# 7. ë¶„ì„ ì‹¤í–‰ ë° ìºì‹± ë¡œì§
# ==============================

def analyze_symbol(item, periods, analyze_patterns, pattern_type_filter, symbol_filter=None): 
    """ë‹¨ì¼ ì¢…ëª©ì„ ë¶„ì„í•˜ê³  í•„í„°ë§ ì¡°ê±´ì— ë§ëŠ”ì§€ í™•ì¸í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    code = item.get("Code") or item.get("code")
    name = item.get("Name") or item.get("name")
    path = DATA_DIR / f"{code}.parquet"

    if not path.exists():
        logging.debug(f"[{code}] ë°ì´í„° íŒŒì¼ ì—†ìŒ.")
        return None

    try:
        df_raw = pd.read_parquet(path)
        if df_raw.index.dtype != 'datetime64[ns]' and 'Date' in df_raw.columns:
            df_raw = df_raw.set_index('Date')
            
        if df_raw.empty or len(df_raw) < 250:
            logging.debug(f"[{code}] ë°ì´í„° ë¶€ì¡± ({len(df_raw)}ì¼).")
            return None

        df_full = calculate_advanced_features(df_raw)
        df_full = add_market_regime_clustering(df_full)
        
        df_analyze = df_full.iloc[-250:].copy() 

        if len(df_analyze) < 200: 
            logging.debug(f"[{code}] ìµœì¢… ë¶„ì„ ë°ì´í„° ë¶€ì¡± ({len(df_analyze)}ì¼).")
            return None

        analysis_results = check_ma_conditions(df_analyze, periods, analyze_patterns)

        # í•„í„°ë§ ë¡œì§ ì ìš©
        is_match = True
        if pattern_type_filter:
            if pattern_type_filter == 'goldencross':
                is_match = analysis_results.get("goldencross_50_200_detected", False)
            elif pattern_type_filter == 'deadcross': 
                is_match = analysis_results.get("deadcross_50_200_detected", False)
            elif pattern_type_filter in ['double_bottom', 'triple_bottom', 'cup_and_handle', 'half_cup', 'long_term_down_trend']:
                status_key = f'pattern_{pattern_type_filter}_status'
                status = analysis_results.get(status_key)
                # DownwardëŠ” ì¥ê¸°í•˜ë½ì¶”ì„¸ ì „ìš© ìƒíƒœ
                is_match = status in ['Breakout', 'Potential', 'Downward']
            elif pattern_type_filter.startswith('regime:'):
                if 'market_regime' in analysis_results:
                    try:
                        target_regime = int(pattern_type_filter.split(':')[1])
                        current_regime = analysis_results['market_regime']
                        is_match = (current_regime == target_regime)
                    except ValueError:
                        is_match = False
                else:
                    is_match = False
            elif pattern_type_filter == 'ma':
                is_match = all(analysis_results.get(f"above_ma{p}", False) for p in periods if p in [20, 50, 200]) 
            elif pattern_type_filter == 'all_below_ma':
                is_match = all(
                    (df_analyze['Close'].iloc[-1] < df_analyze.get(f'SMA_{p}', df_analyze.get(f'ma{p}', 0)).iloc[-1])
                    for p in periods if p in [20, 50, 200]
                )
            else:
                is_match = False

        if pattern_type_filter and not is_match: 
            return None

        if analysis_results:
            analysis_clean = {k: v for k, v in analysis_results.items() if v is not None}
            
            # â˜… ì •ë ¬ ì ìˆ˜ ê²°ì • ë¡œì§
            if pattern_type_filter == 'half_cup':
                sort_score = analysis_clean.get('hc_l_score', 0)
            elif pattern_type_filter == 'long_term_down_trend':
                sort_score = analysis_clean.get('ltd_score', 0)
            else:
                sort_score = analysis_clean.get('market_regime', -1)
            
            return {
                "ticker": code,
                "name": name,
                "technical_conditions": analysis_clean, 
                "sort_score": sort_score 
            }
        return None
    except Exception as e:
        logging.error(f"[ERROR] {code} {name} ë¶„ì„ ì‹¤íŒ¨: {e}\n{traceback.format_exc()}")
        return None

def run_analysis(workers, ma_periods_str, analyze_patterns_flag, pattern_type_filter, top_n, force=False, symbol_filter=None): 
    """ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì´ìš©í•´ ì „ì²´ ì¢…ëª© ë¶„ì„ì„ ì‹¤í–‰í•˜ê³ , ì¼ì¼ ìºì‹±ì„ ì ìš©í•©ë‹ˆë‹¤."""
    
    cleanup_old_cache() 
    
    start_time = time.time()
    periods = [int(p.strip()) for p in ma_periods_str.split(',') if p.strip().isdigit()]

    today_str = datetime.now().strftime("%Y%m%d")
    analyze_patterns = analyze_patterns_flag or (pattern_type_filter not in [None, 'ma', 'all_below_ma'] and not str(pattern_type_filter).startswith('regime:'))
    
    cache_filter_key = f"{pattern_type_filter or 'ma_only'}_{'pattern' if analyze_patterns else 'no_pattern'}"
    cache_key = f"{today_str}_{cache_filter_key.replace(':', '_')}_{top_n}.json" 
    cache_path = CACHE_DIR / cache_key
    
    # ğŸ”¥ ìˆ˜ì •: force ê°€ False ì¼ ë•Œë§Œ ìºì‹œë¥¼ ì½ìŒ (force=True ì´ë©´ ë¬´ì¡°ê±´ ìƒˆë¡œ ë¶„ì„)
    if not force and not symbol_filter and cache_path.exists(): 
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                cached_data = json.load(f)
            sys.stdout.write(json.dumps(cached_data, ensure_ascii=False, indent=None, separators=(',', ':'), cls=CustomJsonEncoder) + "\n")
            sys.stdout.flush()
            sys.exit(0)
        except Exception: pass

    if 50 not in periods: periods.append(50)
    if 200 not in periods: periods.append(200)

    items = load_listing()
    if symbol_filter:
        items = [item for item in items if (item.get("Code") or item.get("code")) == symbol_filter]
    
    initial_item_count = len(items) 
    if initial_item_count == 0:
        safe_print_json({"error": "DATA_EMPTY"}, status_code=1)
        return

    results = []
    processed_count = 0

    with ThreadPoolExecutor(max_workers=workers) as executor:
        future_to_item = {
            executor.submit(analyze_symbol, item, periods, analyze_patterns, pattern_type_filter): item
            for item in items
        }

        for future in as_completed(future_to_item):
            processed_count += 1
            progress_percent = round((processed_count / initial_item_count) * 100, 2) 
            sys.stdout.write(json.dumps({"mode": "progress", "progress_percent": progress_percent}, ensure_ascii=False) + "\n")
            sys.stdout.flush()

            try:
                r = future.result()
                if r: results.append(r)
            except Exception: pass

    # â˜… ì •ë ¬ ë¡œì§: sort_score ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ
    results.sort(key=lambda x: x.get('sort_score', -1), reverse=True)
    final_results = results[:top_n] if top_n > 0 else results
    
    for r in final_results:
        r.pop('sort_score', None)

    final_output = {
        "results": final_results,
        "mode": "analyze_result",
        "filter": pattern_type_filter or 'ma_only'
    }
    
    if not symbol_filter:
        try:
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(final_output, f, ensure_ascii=False, cls=CustomJsonEncoder, indent=None, separators=(',', ':'))
        except Exception: pass

    safe_print_json(final_output, status_code=0)


# ==============================
# 8. ì°¨íŠ¸ ìƒì„± ë¡œì§
# ==============================

def generate_chart(symbol, ma_periods_str, chart_period):
    """ë‹¨ì¼ ì¢…ëª©ì˜ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ Chart.js JSON í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    code = symbol
    name = get_stock_name(code)
    periods = [int(p.strip()) for p in ma_periods_str.split(',') if p.strip().isdigit()] 
    path = DATA_DIR / f"{code}.parquet"

    if not path.exists():
        safe_print_json({"error": "FILE_NOT_FOUND"}, status_code=1)
        return

    try:
        df = pd.read_parquet(path)
        if df.index.dtype != 'datetime64[ns]' and 'Date' in df.columns:
            df = df.set_index('Date')

        df_full = calculate_advanced_features(df)
        df_for_chart = df_full.iloc[-chart_period:].copy()

        ohlcv_data = [{"x": idx.strftime('%Y-%m-%d'), "o": r['Open'], "h": r['High'], "l": r['Low'], "c": r['Close'], "v": r['Volume']} for idx, r in df_for_chart.iterrows()]
        
        ma_data = {}
        for p in periods:
            ma_col = f'SMA_{p}'
            if ma_col not in df_for_chart.columns:
                df_for_chart[ma_col] = df_for_chart['Close'].rolling(window=p, min_periods=1).mean()
            ma_data[f"MA{p}"] = [{"x": idx.strftime('%Y-%m-%d'), "y": r[ma_col]} for idx, r in df_for_chart.iterrows() if not pd.isna(r[ma_col])]
        
        macd_data = {
            "MACD": [{"x": i.strftime('%Y-%m-%d'), "y": r['MACD']} for i, r in df_for_chart.iterrows()],
            "Signal": [{"x": i.strftime('%Y-%m-%d'), "y": r['MACD_Signal']} for i, r in df_for_chart.iterrows()],
            "Histogram": [{"x": i.strftime('%Y-%m-%d'), "y": r['MACD_Hist']} for i, r in df_for_chart.iterrows()]
        }

        # íŒ¨í„´ ë° í¬ë¡œìŠ¤ ì§€ì 
        peaks_all, troughs_all = find_peaks_and_troughs(df_full)
        _, db_neckline, db_status, _ = find_double_bottom(df_full, troughs_all)
        _, tb_neckline, tb_status, _ = find_triple_bottom(df_full, troughs_all)
        _, ch_neckline, ch_status, _ = find_cup_and_handle(df_full, peaks_all, troughs_all)
        _, hc_neckline, hc_status, _ = find_half_cup_waist(df_full)
        _, ltd_neckline, ltd_status, _ = find_long_term_down_trend(df_full)

        pattern_data = []
        today_date = df_full.index[-1].strftime('%Y-%m-%d')
        for p_name, p_neck, p_stat in [
            ("DoubleBottom", db_neckline, db_status), 
            ("TripleBottom", tb_neckline, tb_status), 
            ("CupAndHandle", ch_neckline, ch_status), 
            ("HalfCup", hc_neckline, hc_status),
            ("LongTermDown", ltd_neckline, ltd_status)
        ]:
            if p_neck: pattern_data.append({"x": today_date, "y": p_neck, "type": p_name, "status": p_stat})

        safe_print_json({
            "ticker": code, "name": name, "mode": "chart_data",
            "ohlcv_data": ohlcv_data, "ma_data": ma_data, "macd_data": macd_data, "pattern_points": pattern_data
        }, status_code=0)

    except Exception:
        safe_print_json({"error": "CHART_FAIL"}, status_code=1)


def main():
    parser = argparse.ArgumentParser(description="ì£¼ì‹ ë°ì´í„° ë¶„ì„ ë° ì°¨íŠ¸ ë°ì´í„° ìƒì„± ìŠ¤í¬ë¦½íŠ¸")
    parser.add_argument("--mode", type=str, required=True, choices=['analyze', 'chart'])
    parser.add_argument("--workers", type=int, default=os.cpu_count() * 2)
    parser.add_argument("--ma_periods", type=str, default="20,50,200")
    parser.add_argument("--chart_period", type=int, default=250)
    parser.add_argument("--symbol", type=str)
    parser.add_argument("--analyze_patterns", action="store_true")
    
    # ğŸ”¥ force ì¸ì ì¶”ê°€ (ì´ê²Œ ìˆì–´ì•¼ ìë°”ì—ì„œ ë³´ë‚¸ --forceë¥¼ ì¸ì‹í•¨)
    parser.add_argument("--force", action="store_true", help="ìºì‹œë¥¼ ë¬´ì‹œí•˜ê³  ê°•ì œ ë¶„ì„ ì‹¤í–‰")
    
    parser.add_argument("--pattern_type", type=str, choices=['ma', 'all_below_ma', 'double_bottom', 'triple_bottom', 'cup_and_handle', 'half_cup', 'long_term_down_trend', 'goldencross', 'deadcross', 'regime:0', 'regime:1', 'regime:2', 'regime:3'])
    parser.add_argument("--debug", action="store_true")
    parser.add_argument("--top_n", type=int, default=10)
    
    args = parser.parse_args()
    setup_env(log_level=logging.DEBUG if args.debug else logging.INFO) 
    
    if args.mode == 'analyze':
        analyze_patterns_flag = args.analyze_patterns or (args.pattern_type not in [None, 'ma', 'all_below_ma'] and not (args.pattern_type and str(args.pattern_type).startswith('regime:')))
        # ğŸ”¥ args.force ì¶”ê°€ ì „ë‹¬
        run_analysis(args.workers, args.ma_periods, analyze_patterns_flag, args.pattern_type, args.top_n, args.force, args.symbol)
    elif args.mode == 'chart':
        if not args.symbol: return
        generate_chart(args.symbol, args.ma_periods, args.chart_period)

if __name__ == "__main__":
    main()